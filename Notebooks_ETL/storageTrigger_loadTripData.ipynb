{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f3d5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import traceback\n",
    "import re\n",
    "import yaml\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "from google.oauth2 import service_account\n",
    "from google.cloud.exceptions import NotFound\n",
    "\n",
    "# Configuración del proyecto y dataset de BigQuery\n",
    "PROJECT_ID = 'ultimate-realm-388922'\n",
    "BQ_DATASET = 'taxis_nyc'  # Nombre del dataset\n",
    "\n",
    "# Carga la configuración de esquemas desde el archivo YAML\n",
    "with open(\"tripdata_schema.yaml\") as schema_file:\n",
    "    config = yaml.load(schema_file, Loader=yaml.FullLoader)\n",
    "\n",
    "# Inicializa clientes para Cloud Storage y BigQuery\n",
    "storage_client = storage.Client()\n",
    "bigquery_client = bigquery.Client()\n",
    "\n",
    "# Configuración inicial para los trabajos de carga en BigQuery\n",
    "job_config = bigquery.LoadJobConfig()\n",
    "\n",
    "# Las funciones auxiliares van aquí (check_and_create_table, load_parquet_into_bigquery, create_bigquery_schema)\n",
    "def check_and_create_table(table_name, table_schema):\n",
    "    \"\"\" Verifica si una tabla existe en BigQuery y la crea si no existe. \"\"\"\n",
    "    table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{table_name}\"\n",
    "\n",
    "    try:\n",
    "        bigquery_client.get_table(table_id)\n",
    "    except NotFound:\n",
    "        logging.warning(f'Creating table: {table_name}')\n",
    "        schema = create_bigquery_schema(table_schema)\n",
    "        table = bigquery.Table(table_id, schema=schema)\n",
    "        bigquery_client.create_table(table)\n",
    "        print(f\"Created table {table_id}\")\n",
    "\n",
    "def load_parquet_into_bigquery(bucket_name, file_name, table_schema, table_name):\n",
    "    \"\"\" Carga un archivo Parquet desde Cloud Storage a una tabla de BigQuery. \"\"\"\n",
    "    uri = f'gs://{bucket_name}/{file_name}'\n",
    "    table_id = f\"{PROJECT_ID}.{BQ_DATASET}.{table_name}\"\n",
    "\n",
    "    schema = create_bigquery_schema(table_schema)\n",
    "    job_config.schema = schema\n",
    "    job_config.source_format = bigquery.SourceFormat.PARQUET\n",
    "    job_config.write_disposition = bigquery.WriteDisposition.WRITE_APPEND\n",
    "\n",
    "    load_job = bigquery_client.load_table_from_uri(uri, table_id, job_config=job_config)\n",
    "    load_job.result()\n",
    "    print(f\"La tarea de carga para {uri} hacia {table_id} ha finalizado.\")\n",
    "\n",
    "def create_bigquery_schema(table_schema_yaml):\n",
    "    \"\"\" Convierte el esquema YAML a un esquema de BigQuery. \"\"\"\n",
    "    schema = []\n",
    "    for column in table_schema_yaml:\n",
    "        schema_field = bigquery.SchemaField(column['name'], column['type'], column.get('mode', 'NULLABLE'))\n",
    "        schema.append(schema_field)\n",
    "        if column['type'] == 'RECORD':\n",
    "            schema_field._fields = create_bigquery_schema(column['fields'])\n",
    "    return schema\n",
    "\n",
    "def gcf_trigger(data, context):\n",
    "    \"\"\" Función para procesar un archivo subido a Cloud Storage y cargarlo en BigQuery. \"\"\"\n",
    "    bucket_name = data['bucket']\n",
    "    file_name = data['name']\n",
    "    time_created = data['timeCreated']\n",
    "\n",
    "    print(f\"Bucket: {bucket_name}, File: {file_name}, Time Created: {time_created}\")\n",
    "\n",
    "    # Comprobar si el archivo está en una de las carpetas específicas\n",
    "    if (file_name.startswith('pre-data/taxis-amarillos/') or \n",
    "        file_name.startswith('pre-data/taxis-verdes/') or \n",
    "        file_name.startswith('pre-data/taxis-alquiler/')):\n",
    "        try:\n",
    "            for table_config in config:\n",
    "                table_name = table_config['name']  # Asegúrate de que config es una lista de diccionarios\n",
    "                if re.search(table_name.replace('_', '-'), file_name) or re.search(table_name, file_name):\n",
    "                    table_schema = table_config['schema']\n",
    "                    check_and_create_table(table_name, table_schema)\n",
    "                    load_parquet_into_bigquery(bucket_name, file_name, table_schema, table_name)\n",
    "        except Exception as e:\n",
    "            print(f'Error al procesar el archivo: {traceback.format_exc()}')\n",
    "    else:\n",
    "        print(f\"Ignorando archivo fuera de las carpetas especificadas: {file_name}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
